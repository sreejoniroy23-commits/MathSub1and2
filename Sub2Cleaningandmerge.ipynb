{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99697c2",
   "metadata": {},
   "source": [
    "Sreejoni Roy\n",
    "**Reading the Parquet Files**\n",
    "\n",
    "In this step, I loaded the two cleaned datasets into Python using pd.read_parquet().\n",
    "\n",
    "df_items contains all the item feature information.\n",
    "\n",
    "df_users contains the user transaction records.\n",
    "\n",
    "After loading, I printed the column types and the first few rows from each dataset to quickly check the structure and make sure everything was read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b379333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEM FEATURES COLUMNS:\n",
      "\n",
      "Articlenr                  int64\n",
      "EF000001                 float64\n",
      "EF000002                 float64\n",
      "EF000003                 float64\n",
      "EF000004                 float64\n",
      "                          ...   \n",
      "EFUK0017                 float64\n",
      "EFUK0019                 float64\n",
      "ETIM                     float64\n",
      "statistiek_hoofdgroep     object\n",
      "statistiek_subgroep       object\n",
      "Length: 4028, dtype: object\n",
      "\n",
      "Sample rows from item features:\n",
      "\n",
      "   Articlenr  EF000001  EF000002  EF000003  EF000004  EF000005  EF000006  \\\n",
      "0   34581962       6.0       NaN       NaN       NaN       NaN       NaN   \n",
      "1   35764300       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "2   35823358       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "3   28405226       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "4   35622440       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "   EF000007  EF000008  EF000010  ...  EFDE0031  EFDE0032  EFFR0001  EFFR0002  \\\n",
      "0       NaN       6.2       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "1       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "2       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "3       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "4       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "\n",
      "   EFNL0014  EFUK0017  EFUK0019   ETIM  statistiek_hoofdgroep  \\\n",
      "0       NaN       NaN       NaN   26.0                      A   \n",
      "1       NaN       NaN       NaN   30.0                      A   \n",
      "2       NaN       NaN       NaN   31.0                      A   \n",
      "3       NaN       NaN       NaN  321.0                      B   \n",
      "4       NaN       NaN       NaN   14.0                      C   \n",
      "\n",
      "   statistiek_subgroep  \n",
      "0                   x1  \n",
      "1                   x2  \n",
      "2                   x3  \n",
      "3                   y1  \n",
      "4                   y2  \n",
      "\n",
      "[5 rows x 4028 columns]\n",
      "\n",
      "\n",
      "USER TRANSACTIONS COLUMNS:\n",
      "\n",
      "customer                      int64\n",
      "invoice                       int64\n",
      "invoice_date         datetime64[ns]\n",
      "articlenr                     int64\n",
      "volume                        int64\n",
      "sum1                        float64\n",
      "sum2                        float64\n",
      "postcode_delivery             int64\n",
      "postcode_customer             int64\n",
      "channel                      object\n",
      "dtype: object\n",
      "\n",
      "Sample rows from user transactions:\n",
      "\n",
      "   customer  invoice invoice_date  articlenr  volume     sum1     sum2  \\\n",
      "0      2284  1187467   2020-01-02   32879960      24   343.73   362.30   \n",
      "1      2284  1187467   2020-01-02   32879962      60  1067.64  1119.96   \n",
      "2      2284  1187467   2020-01-02   32841810     120  2214.00  2408.40   \n",
      "3      1704  1191447   2020-01-17   36866802       1    21.52    19.32   \n",
      "4        45  1187691   2020-01-03   34360272       4    28.91    28.49   \n",
      "\n",
      "   postcode_delivery  postcode_customer    channel  \n",
      "0              80413              80413  Tel 86041  \n",
      "1              80413              80413  Tel 86041  \n",
      "2              80413              80413  Tel 86041  \n",
      "3              81724              81724  OTC 81812  \n",
      "4              81796              81796  Web Order  \n"
     ]
    }
   ],
   "source": [
    "#Reading the parquet file for features and transcations\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_items = pd.read_parquet(\"Tao Yin_Item_features.parquet\")\n",
    "df_users = pd.read_parquet(\"TaoYin_User_Transactions_v2.parquet\")\n",
    "\n",
    "print(\"ITEM FEATURES COLUMNS:\\n\")\n",
    "print(df_items.dtypes)\n",
    "print(\"\\nSample rows from item features:\\n\")\n",
    "print(df_items.head())\n",
    "\n",
    "print(\"\\n\\nUSER TRANSACTIONS COLUMNS:\\n\")\n",
    "print(df_users.dtypes)\n",
    "print(\"\\nSample rows from user transactions:\\n\")\n",
    "print(df_users.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e0d46",
   "metadata": {},
   "source": [
    "**Understanding the Output**\n",
    "\n",
    "The item features table has 4028 columns, mostly numeric feature codes (like EF000001, EFDE0031, etc.). The sample rows show that many of these feature values are missing (NaN), which is expected because not every item has every feature. The last columns (statistiek_hoofdgroep and statistiek_subgroep) are categorical.\n",
    "\n",
    "The user transactions table looks much cleaner. It includes the basic transaction info such as customer ID, invoice, date, product number, volume, and the two price columns (sum1 and sum2). The sample rows help confirm the structure: repeated invoices, proper date format, and the sales channel as a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac600f",
   "metadata": {},
   "source": [
    "**Cleaning the Item Features Dataset**\n",
    "\n",
    "In this section, I loaded the raw item features parquet file and prepared it for analysis. I first inspected the structure (shape, columns, sample rows) and made sure the product key Articlenr exists. Then, I created a lowercase version of the key (articlenr) so it would match the transactions table later.\n",
    "The main goal here was to remove duplicates, fix data types, handle missing values, drop useless columns, and reorganise everything into a clean, analysis-ready dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24939b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd              # pandas for dataframes\n",
    "import numpy as np               # numpy for numeric operations\n",
    "from pathlib import Path         # Path for safe file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e492dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the folder where my files are located \n",
    "DATA_PATH = Path(\".\")\n",
    "\n",
    "# The raw file and the cleaned output file\n",
    "RAW_FEATURES = DATA_PATH / \"Tao Yin_Item_features.parquet\"          # raw features file (input)\n",
    "CLEAN_FEATURES = DATA_PATH / \"item_features_clean.parquet\"          # cleaned features file (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab8ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (rows, cols): (168939, 4028)\n",
      "\n",
      "First 20 columns: ['Articlenr', 'EF000001', 'EF000002', 'EF000003', 'EF000004', 'EF000005', 'EF000006', 'EF000007', 'EF000008', 'EF000010', 'EF000012', 'EF000013', 'EF000014', 'EF000015', 'EF000016', 'EF000017', 'EF000018', 'EF000019', 'EF000022', 'EF000024']\n",
      "\n",
      "Preview rows:\n",
      "    Articlenr  EF000001  EF000002  EF000003  EF000004  EF000005  EF000006  \\\n",
      "0   34581962       6.0       NaN       NaN       NaN       NaN       NaN   \n",
      "1   35764300       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "2   35823358       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "   EF000007  EF000008  EF000010  ...  EFDE0031  EFDE0032  EFFR0001  EFFR0002  \\\n",
      "0       NaN       6.2       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "1       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "2       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "\n",
      "   EFNL0014  EFUK0017  EFUK0019  ETIM  statistiek_hoofdgroep  \\\n",
      "0       NaN       NaN       NaN  26.0                      A   \n",
      "1       NaN       NaN       NaN  30.0                      A   \n",
      "2       NaN       NaN       NaN  31.0                      A   \n",
      "\n",
      "   statistiek_subgroep  \n",
      "0                   x1  \n",
      "1                   x2  \n",
      "2                   x3  \n",
      "\n",
      "[3 rows x 4028 columns]\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_parquet(RAW_FEATURES)                            # loading the parquet into a DataFrame called 'features'\n",
    "\n",
    "print(\"Shape (rows, cols):\", features.shape)                        # checking the size of rows and columns\n",
    "print(\"\\nFirst 20 columns:\", list(features.columns[:20]))           # checking column names\n",
    "print(\"\\nPreview rows:\\n\", features.head(3))                        # checking rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244b96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"Articlenr\" in features.columns, \"Articlenr not found!\"      \n",
    "# ensuring that the key exists\n",
    "\n",
    "features[\"articlenr\"] = features[\"Articlenr\"].astype(\"int64\")       \n",
    "# creating a lowercase merge key to match with the transactions files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1205260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicates removed (by Articlenr): 2711\n"
     ]
    }
   ],
   "source": [
    "rows_before = features.shape[0]                                   \n",
    "  # remembering the original row count\n",
    "features = features.drop_duplicates(subset=[\"Articlenr\"])           \n",
    "# keeping only the first row per unique product id\n",
    "rows_after = features.shape[0]                                     \n",
    " # finding out the new row count\n",
    "\n",
    "print(f\"\\nDuplicates removed (by Articlenr): {rows_before - rows_after}\") \n",
    " # reports how many were duplicates were removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b148579",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"statistiek_hoofdgroep\", \"statistiek_subgroep\"]:       \n",
    "     # looping through the two categorical columns\n",
    "    if col in features.columns:                                     # this code only acts if the column exists\n",
    "        features[col] = features[col].astype(\"string\")              # making it text \n",
    "        features[col] = features[col].str.strip()                   # removing any accidental leading spaces\n",
    "        features[col] = features[col].astype(\"category\")            # storing it as category as i saves memory\n",
    "\n",
    "if \"ETIM\" in features.columns:                                      # ETIM is the numeric descriptor\n",
    "    features[\"ETIM\"] = pd.to_numeric(features[\"ETIM\"],              # converting all to numbers and invalid data will become NaN\n",
    "                                      errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee9ee645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-15 most-missing columns (%):\n",
      "EF006634    100.0\n",
      "EF012526    100.0\n",
      "EF009515    100.0\n",
      "EF002963    100.0\n",
      "EF008203    100.0\n",
      "EF012524    100.0\n",
      "EF008875    100.0\n",
      "EF006273    100.0\n",
      "EF006781    100.0\n",
      "EF008214    100.0\n",
      "EF007342    100.0\n",
      "EF007340    100.0\n",
      "EF007344    100.0\n",
      "EF003984    100.0\n",
      "EF003312    100.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_pct = features.isna().mean().sort_values(ascending=False)  \n",
    " # percentage missing per column \n",
    "\n",
    "print(\"\\nTop-15 most-missing columns (%):\")\n",
    "print((missing_pct.head(15) * 100).round(1))                       \n",
    " # showing the most missed columns \n",
    "\n",
    "KEEP_COLS = {                                                       \n",
    "     # columns not to drop because I need them for the anaylsis\n",
    "    \"Articlenr\", \"articlenr\", \"ETIM\",\n",
    "    \"statistiek_hoofdgroep\", \"statistiek_subgroep\"\n",
    "}\n",
    "\n",
    "THRESH = 0.80                                                       # drop columns with >80% missing (tunable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f26746b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = []                                                       \n",
    "  # collecting the columns to drop them in this list/place.\n",
    "\n",
    "for col in features.columns:                                         # scaning all the columns\n",
    "    if col in KEEP_COLS:                                             # skipping the essential columns\n",
    "        continue\n",
    "    if missing_pct.get(col, 0.0) > THRESH:                           # if more than 80% is missing then that column will be dropped.\n",
    "        to_drop.append(col)                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0cd7357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns to drop (> 80% missing): 4016\n",
      "Sample to drop: ['EF000001', 'EF000002', 'EF000003', 'EF000004', 'EF000005', 'EF000006', 'EF000012', 'EF000013', 'EF000014', 'EF000015']\n",
      "Shape after dropping high-missing columns: (166228, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nColumns to drop (> {int(THRESH*100)}% missing):\", len(to_drop)) \n",
    " # report how many will go\n",
    "print(\"Sample to drop:\", to_drop[:10])                             \n",
    "  # peek a few\n",
    "\n",
    "features = features.drop(columns=to_drop)                            # drop them the not-needed/missing columns\n",
    "print(\"Shape after dropping high-missing columns:\", features.shape)  # new size of columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6643d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "const_cols = []                                                     \n",
    " # collecting the constant columns in this list/place.\n",
    "\n",
    "for col in features.columns:                                         # checking each remaining column\n",
    "    if col in KEEP_COLS:                                             # Do not drop the essential columns\n",
    "        continue\n",
    "    if features[col].nunique(dropna=True) <= 1:                      # 0 or 1 unique then its not needed.\n",
    "        const_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae8ed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constant/no-variance columns: 0\n",
      "Sample constants: []\n",
      "Shape after dropping constants: (166228, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConstant/no-variance columns:\", len(const_cols))            # Number of constants that were found\n",
    "print(\"Sample constants:\", const_cols[:10])                          # seeing a sample \n",
    "\n",
    "features = features.drop(columns=const_cols)                         # removing the constants\n",
    "print(\"Shape after dropping constants:\", features.shape)             # new size of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d70f9348",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_cols = [c for c in features.columns if c.startswith(\"EF\")]       \n",
    "base_cols = [                                                     \n",
    "       # the Important columns first\n",
    "    \"Articlenr\", \"articlenr\", \"ETIM\",\n",
    "    \"statistiek_hoofdgroep\", \"statistiek_subgroep\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06942ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final column order set. First 12 columns:\n",
      "['Articlenr', 'articlenr', 'ETIM', 'statistiek_hoofdgroep', 'statistiek_subgroep', 'EF000007', 'EF000008', 'EF000010', 'EF000040', 'EF000049', 'EF000139', 'EF002169']\n"
     ]
    }
   ],
   "source": [
    "ordered_cols = [c for c in base_cols if c in features.columns] + ef_cols \n",
    " # keeping existing ones \n",
    "features = features[ordered_cols]                                \n",
    "   # putting it in order\n",
    "\n",
    "print(\"\\nFinal column order set. First 12 columns:\")\n",
    "print(features.columns[:12].tolist())                            \n",
    "   # seeing a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29b75cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ETIM\" in features.columns:                                      # finding out the negatives and replacing them \n",
    "    neg_count = (features[\"ETIM\"] < 0).sum(skipna=True)             \n",
    "    if neg_count > 0:                                               \n",
    "        print(f\"\\nETIM negatives found and set to NaN: {neg_count}\")\n",
    "        features.loc[features[\"ETIM\"] < 0, \"ETIM\"] = np.nan         # replacig the negatives with NaN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c508db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved cleaned features → C:\\Users\\sreej\\Downloads\\stats\\sub-assignments\\SUB2\\item_features_clean.parquet\n",
      "Final shape: (166228, 13)\n"
     ]
    }
   ],
   "source": [
    "features.to_parquet(CLEAN_FEATURES, index=False)                    \n",
    "print(\"\\nSaved cleaned features →\", CLEAN_FEATURES.resolve())      \n",
    "print(\"Final shape:\", features.shape)                               \n",
    "#making the new cleaned features file and saving it in the vs code folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6ec181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLEANING SUMMARY: {'rows': 166228, 'cols': 13, 'kept_main_cols': ['Articlenr', 'articlenr', 'ETIM', 'statistiek_hoofdgroep', 'statistiek_subgroep'], 'num_EF_cols': 8}\n"
     ]
    }
   ],
   "source": [
    "summary = {                                                         # just checking overall\n",
    "    \"rows\": features.shape[0],                                      # number of products\n",
    "    \"cols\": features.shape[1],                                      # number of columns after the cleaning\n",
    "    \"kept_main_cols\": [c for c in base_cols if c in features.columns],  # what was kept\n",
    "    \"num_EF_cols\": sum(c.startswith(\"EF\") for c in features.columns)    # the count of the remaining EF features\n",
    "}\n",
    "print(\"\\nCLEANING SUMMARY:\", summary)                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0be64",
   "metadata": {},
   "source": [
    "**After cleaning the features dataset**\n",
    "\n",
    "After completing all cleaning steps, the dataset is now much smaller and more usable.\n",
    "Duplicate items were removed, categorical columns were cleaned, ETIM was converted to a proper numeric type, and EF-columns with too many missing values or no variance were dropped. Finally, the important columns were reordered and the cleaned file was saved.\n",
    "\n",
    "Final output:\n",
    "\n",
    "-Rows: 166,228\n",
    "\n",
    "-Columns: 13\n",
    "\n",
    "-Main columns kept: Articlenr, articlenr, ETIM, statistiek_hoofdgroep, statistiek_subgroep\n",
    "\n",
    "-Remaining EF-feature columns: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab69e6",
   "metadata": {},
   "source": [
    "**Cleaning the Transactions Dataset**\n",
    "\n",
    "In this part, I loaded the raw transactions file and started preparing it for analysis.\n",
    "First, I inspected the basic structure (rows, columns, and sample data), then converted all column names to lowercase and fixed their data types (integers, dates, numeric amounts, etc.).\n",
    "The goal here was to standardise the dataset, remove duplicates, clean messy fields like channel, and check for invalid or missing values before using the data for ANOVA (anova for sub-question 2) and merging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2c8082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd          # 'pd' is a short alias for the pandas library \n",
    "import numpy as np           # 'np' is a short alias for numpy\n",
    "from pathlib import Path     # 'Path' helps build file paths in a safe/folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9a493e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a base path for the transcations file. Path(\".\") means \"the current folder\".\n",
    "DATA_PATH = Path(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "599eb41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building full paths to the input/output files.\n",
    "# The slash \"/\" is overloaded by Path to mean \"join path parts\".\n",
    "RAW_TX = DATA_PATH / \"TaoYin_User_Transactions_v2.parquet\"\n",
    "CLEAN_TX = DATA_PATH / \"transactions_clean.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ade0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pd.read_parquet reads the parquet file into the DataFrame.\n",
    "tx = pd.read_parquet(RAW_TX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71ca10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (rows, cols): (492731, 10)\n",
      "\n",
      "First 20 columns: ['customer', 'invoice', 'invoice_date', 'articlenr', 'volume', 'sum1', 'sum2', 'postcode_delivery', 'postcode_customer', 'channel']\n",
      "\n",
      "Preview:\n",
      "   customer  invoice invoice_date  articlenr  volume     sum1     sum2  \\\n",
      "0      2284  1187467   2020-01-02   32879960      24   343.73   362.30   \n",
      "1      2284  1187467   2020-01-02   32879962      60  1067.64  1119.96   \n",
      "2      2284  1187467   2020-01-02   32841810     120  2214.00  2408.40   \n",
      "3      1704  1191447   2020-01-17   36866802       1    21.52    19.32   \n",
      "4        45  1187691   2020-01-03   34360272       4    28.91    28.49   \n",
      "\n",
      "   postcode_delivery  postcode_customer    channel  \n",
      "0              80413              80413  Tel 86041  \n",
      "1              80413              80413  Tel 86041  \n",
      "2              80413              80413  Tel 86041  \n",
      "3              81724              81724  OTC 81812  \n",
      "4              81796              81796  Web Order  \n"
     ]
    }
   ],
   "source": [
    "# shape uses a dot \".\" to access an \"attribute\" (a stored property) of the DataFrame.\n",
    "# It returns a (rows, columns) tuple. The comma \",\" separates items in the tuple.\n",
    "#printing the number of rows and columns\n",
    "print(\"Shape (rows, cols):\", tx.shape)\n",
    "\n",
    "# .columns returns all column names. [:20] slices the first 20 items.\n",
    "print(\"\\nFirst 20 columns:\", list(tx.columns[:20]))\n",
    "\n",
    "# .head(5) prints the first 5 rows.\n",
    "print(\"\\nPreview:\")\n",
    "print(tx.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d73f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy so i don't accidentally overwrite the original DataFrame in the memory.\n",
    "tx = tx.copy()\n",
    "\n",
    "# Converting column names to lowercase for consistency.\n",
    "# .columns accesses the list-like Index of names; .str.lower() converting each name to lower-case.\n",
    "tx.columns = tx.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54453e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All expected columns present?: True\n"
     ]
    }
   ],
   "source": [
    "# expected list the columns.\n",
    "expected = [\n",
    "    \"customer\", \"invoice\", \"invoice_date\", \"articlenr\", \"volume\",\n",
    "    \"sum1\", \"sum2\", \"postcode_delivery\", \"postcode_customer\", \"channel\"\n",
    "]\n",
    "print(\"\\nAll expected columns present?:\", set(expected).issubset(set(tx.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ac57712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring that all the key columns have the correct data types.\n",
    "# .astype(\"int64\") converts the series to 64-bit integer.\n",
    "tx[\"customer\"] = tx[\"customer\"].astype(\"int64\")\n",
    "tx[\"invoice\"] = tx[\"invoice\"].astype(\"int64\")\n",
    "tx[\"articlenr\"] = tx[\"articlenr\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "218435f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates. pd.to_datetime(...) converts text to datetime objects.\n",
    "tx[\"invoice_date\"] = pd.to_datetime(tx[\"invoice_date\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97335f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric money-type fields: coerce invalid strings to NaN (missing).\n",
    "tx[\"sum1\"] = pd.to_numeric(tx[\"sum1\"], errors=\"coerce\")\n",
    "tx[\"sum2\"] = pd.to_numeric(tx[\"sum2\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be741c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume is quantity; making it an integer if it is in whole numbers. If not guaranteed, keeping it as a float.\n",
    "tx[\"volume\"] = pd.to_numeric(tx[\"volume\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d613e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postcodes are codes; they seem numeric in file/data. Keep as int64 if not missing.\n",
    "# If there are  missing values, using Int64 (nullable integer) instead of int64.\n",
    "tx[\"postcode_delivery\"]  = pd.to_numeric(tx[\"postcode_delivery\"], errors=\"coerce\").astype(\"Int64\")\n",
    "tx[\"postcode_customer\"]  = pd.to_numeric(tx[\"postcode_customer\"], errors=\"coerce\").astype(\"Int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8d05ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channels are text; normalize the whitespace and keeping it as a string (pandas' string dtype).\n",
    "tx[\"channel\"] = tx[\"channel\"].astype(\"string\").str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da483ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting rows before dropping the duplicates.\n",
    "before = tx.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9788cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate rows removed: 17534\n"
     ]
    }
   ],
   "source": [
    "# .drop_duplicates() removes rows that are exactly the same across all columns.\n",
    "# subset=[...] restricts the comparison to specific columns (optional).\n",
    "# keep=\"first\" keeps the first occurrence and drops later duplicates.\n",
    "tx = tx.drop_duplicates(keep=\"first\")\n",
    "\n",
    "after = tx.shape[0]\n",
    "print(f\"\\nDuplicate rows removed: {before - after}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2b48952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After validity filters, shape: (475116, 10)\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows with missing essential fields:\n",
    "# .dropna(subset=[...]) removes rows if any of those columns have NaN.\n",
    "tx = tx.dropna(subset=[\"invoice_date\", \"articlenr\", \"volume\", \"sum2\", \"channel\"])\n",
    "\n",
    "# Removing any non-sensical values:\n",
    "# volume should be more than 0 (no negative or zero quantities)\n",
    "tx = tx[tx[\"volume\"] > 0]\n",
    "\n",
    "# sum2 is the final amount; it should be more than 0\n",
    "tx = tx[tx[\"sum2\"] > 0]\n",
    "\n",
    "print(\"After validity filters, shape:\", tx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f16c752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Channel counts:\n",
      "channel_clean\n",
      "Tel          144586\n",
      "ERP          127858\n",
      "Web          123713\n",
      "OTC           74888\n",
      "MobileApp      4052\n",
      "Others           19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In the transcation file, channel has values like \"Tel 86041\", \"OTC 81812\", \"Web Order\".\n",
    "# Making a clean label: the first word (letters only) at the start.\n",
    "# .str.extract(r'(^[A-Za-z]+)') uses a regular expression (regex).\n",
    "# r'...' is a raw string so backslashes are not treated specially by Python itself.\n",
    "tx[\"channel_clean\"] = tx[\"channel\"].str.extract(r'(^[A-Za-z]+)')\n",
    "\n",
    "# Some \"Web Order\" may become \"Web\". \n",
    "# The curly braces { } create a dictionary: \"key\": \"value\", entries separated by commas.\n",
    "mapping = {\n",
    "    \"Web\": \"Web\",\n",
    "    \"Tel\": \"Tel\",\n",
    "    \"OTC\": \"OTC\"\n",
    "}\n",
    "# .map(...) replaces values using the mapping; .fillna(...) fills any non-mapped with the original clean value.\n",
    "tx[\"channel_clean\"] = tx[\"channel_clean\"].map(mapping).fillna(tx[\"channel_clean\"])\n",
    "\n",
    "# Making it a categorical variable (good for grouping and saves on memory).\n",
    "tx[\"channel_clean\"] = tx[\"channel_clean\"].astype(\"category\")\n",
    "\n",
    "print(\"\\nChannel counts:\")\n",
    "print(tx[\"channel_clean\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3891397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Winsorization cap (99th percentile) for sum2: 1506.8644999999915\n"
     ]
    }
   ],
   "source": [
    "# For ANOVA on 'sum2', extreme outliers can dominate.\n",
    "# So I created a winsorized version: capping it at top 1% at the 99th percentile.\n",
    "# .quantile(0.99) returns the 99th percentile.\n",
    "p99 = tx[\"sum2\"].quantile(0.99)\n",
    "\n",
    "# .clip(upper=p99) caps values above p99 to p99.\n",
    "tx[\"sum2_winsor\"] = tx[\"sum2\"].clip(upper=p99)\n",
    "\n",
    "print(\"\\nWinsorization cap (99th percentile) for sum2:\", float(p99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49f47f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary stats (sum2 and sum2_winsor):\n",
      "               sum2    sum2_winsor\n",
      "count  4.751160e+05  475116.000000\n",
      "mean   2.656857e+02     113.698032\n",
      "std    8.180267e+04     220.732958\n",
      "min    1.000000e-02       0.010000\n",
      "25%    1.247000e+01      12.470000\n",
      "50%    3.720000e+01      37.200000\n",
      "75%    1.108800e+02     110.880000\n",
      "max    5.599838e+07    1506.864500\n"
     ]
    }
   ],
   "source": [
    "# .describe() gives summary stats; .groupby(...).size() counts per group.\n",
    "print(\"\\nSummary stats (sum2 and sum2_winsor):\")\n",
    "print(tx[[\"sum2\", \"sum2_winsor\"]].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57e8dda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts by channel_clean:\n",
      "channel_clean\n",
      "ERP          127858\n",
      "MobileApp      4052\n",
      "OTC           74888\n",
      "Others           19\n",
      "Tel          144586\n",
      "Web          123713\n",
      "dtype: int64\n",
      "\n",
      "Invoice date range: 2020-01-02 00:00:00 → 2024-09-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sreej\\AppData\\Local\\Temp\\ipykernel_4228\\1989141854.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  print(tx.groupby(\"channel_clean\").size())\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCounts by channel_clean:\")\n",
    "print(tx.groupby(\"channel_clean\").size())\n",
    "\n",
    "# Checking the date range to ensure the period looks right.\n",
    "print(\"\\nInvoice date range:\", tx[\"invoice_date\"].min(), \"→\", tx[\"invoice_date\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4484e730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved cleaned transactions to: C:\\Users\\sreej\\Downloads\\stats\\sub-assignments\\SUB2\\transactions_clean.parquet\n",
      "Final shape: (475116, 12)\n"
     ]
    }
   ],
   "source": [
    "# Saving the cleaned file as a parquet (fast, keeps dtypes). index=False prevents writing row numbers as a column.\n",
    "tx.to_parquet(CLEAN_TX, index=False)\n",
    "\n",
    "print(\"\\nSaved cleaned transactions to:\", CLEAN_TX.resolve())\n",
    "print(\"Final shape:\", tx.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cfb74",
   "metadata": {},
   "source": [
    "**Final Cleaning Summary**\n",
    "\n",
    "After applying all cleaning steps, the transactions dataset is now fully standardised and filtered. Duplicate rows were removed, incorrect or missing values were dropped, and the channel field was cleaned into a usable category. A winsorized version of sum2 was also created to reduce the influence of extreme outliers for ANOVA.\n",
    "Finally, the cleaned dataset was saved as a new parquet file, ready for merging with the item features and for further statistical analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
