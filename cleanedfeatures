#SreejoniRoy

import pandas as pd              # pandas for dataframes
import numpy as np               # numpy for numeric operations
from pathlib import Path         # Path for safe file paths

# setting the folder where my files are located 
DATA_PATH = Path(".")

# The raw file and the cleaned output file
RAW_FEATURES = DATA_PATH / "Tao Yin_Item_features.parquet"          # raw features file (input)
CLEAN_FEATURES = DATA_PATH / "item_features_clean.parquet"          # cleaned features file (output)



features = pd.read_parquet(RAW_FEATURES)                            # loading the parquet into a DataFrame called 'features'

print("Shape (rows, cols):", features.shape)                        # checking the size of rows and columns
print("\nFirst 20 columns:", list(features.columns[:20]))           # checking column names
print("\nPreview rows:\n", features.head(3))                        # checking rows


assert "Articlenr" in features.columns, "Articlenr not found!"      
# ensuring that the key exists

features["articlenr"] = features["Articlenr"].astype("int64")       
# creating a lowercase merge key to match with the transactions files



rows_before = features.shape[0]                                   
  # remembering the original row count
features = features.drop_duplicates(subset=["Articlenr"])           
# keeping only the first row per unique product id
rows_after = features.shape[0]                                     
 # finding out the new row count

print(f"\nDuplicates removed (by Articlenr): {rows_before - rows_after}") 
 # reports how many were duplicates were removed



for col in ["statistiek_hoofdgroep", "statistiek_subgroep"]:       
     # looping through the two categorical columns
    if col in features.columns:                                     # this code only acts if the column exists
        features[col] = features[col].astype("string")              # making it text 
        features[col] = features[col].str.strip()                   # removing any accidental leading spaces
        features[col] = features[col].astype("category")            # storing it as category as i saves memory

if "ETIM" in features.columns:                                      # ETIM is the numeric descriptor
    features["ETIM"] = pd.to_numeric(features["ETIM"],              # converting all to numbers and invalid data will become NaN
                                      errors="coerce")



missing_pct = features.isna().mean().sort_values(ascending=False)  
 # percentage missing per column 

print("\nTop-15 most-missing columns (%):")
print((missing_pct.head(15) * 100).round(1))                       
 # showing the most missed columns 

KEEP_COLS = {                                                       
     # columns not to drop because I need them for the anaylsis
    "Articlenr", "articlenr", "ETIM",
    "statistiek_hoofdgroep", "statistiek_subgroep"
}

THRESH = 0.80                                                       # drop columns with >80% missing (tunable)



to_drop = []                                                       
  # collecting the columns to drop them in this list/place.

for col in features.columns:                                         # scaning all the columns
    if col in KEEP_COLS:                                             # skipping the essential columns
        continue
    if missing_pct.get(col, 0.0) > THRESH:                           # if more than 80% is missing then that column will be dropped.
        to_drop.append(col)                                          

print(f"\nColumns to drop (> {int(THRESH*100)}% missing):", len(to_drop)) 
 # report how many will go
print("Sample to drop:", to_drop[:10])                             
  # peek a few

features = features.drop(columns=to_drop)                            # drop them the not-needed/missing columns
print("Shape after dropping high-missing columns:", features.shape)  # new size of columns



const_cols = []                                                     
 # collecting the constant columns in this list/place.

for col in features.columns:                                         # checking each remaining column
    if col in KEEP_COLS:                                             # Do not drop the essential columns
        continue
    if features[col].nunique(dropna=True) <= 1:                      # 0 or 1 unique then its not needed.
        const_cols.append(col)

print("\nConstant/no-variance columns:", len(const_cols))            # Number of constants that were found
print("Sample constants:", const_cols[:10])                          # seeing a sample 

features = features.drop(columns=const_cols)                         # removing the constants
print("Shape after dropping constants:", features.shape)             # new size of the data



ef_cols = [c for c in features.columns if c.startswith("EF")]       
base_cols = [                                                     
       # the Important columns first
    "Articlenr", "articlenr", "ETIM",
    "statistiek_hoofdgroep", "statistiek_subgroep"
]

ordered_cols = [c for c in base_cols if c in features.columns] + ef_cols 
 # keeping existing ones 
features = features[ordered_cols]                                
   # putting it in order

print("\nFinal column order set. First 12 columns:")
print(features.columns[:12].tolist())                            
   # seeing a sample



if "ETIM" in features.columns:                                      # finding out the negatives and replacing them 
    neg_count = (features["ETIM"] < 0).sum(skipna=True)             
    if neg_count > 0:                                               
        print(f"\nETIM negatives found and set to NaN: {neg_count}")
        features.loc[features["ETIM"] < 0, "ETIM"] = np.nan         # replacig the negatives with NaN



features.to_parquet(CLEAN_FEATURES, index=False)                    
print("\nSaved cleaned features â†’", CLEAN_FEATURES.resolve())      
print("Final shape:", features.shape)                               
#making the new cleaned features file and saving it in the vs code folder.



summary = {                                                         # just checking overall
    "rows": features.shape[0],                                      # number of products
    "cols": features.shape[1],                                      # number of columns after the cleaning
    "kept_main_cols": [c for c in base_cols if c in features.columns],  # what was kept
    "num_EF_cols": sum(c.startswith("EF") for c in features.columns)    # the count of the remaining EF features
}
print("\nCLEANING SUMMARY:", summary)                              


#I loaded the item features file/data and validated the product key (Articlenr). 
#Then created a lowercase articlenr to match the transactions file/data.
#To clean, I removed any duplicate products, cleaned category fields, and converted the ETIM to numeric.
#I found and cleaned missingness and dropped EF-feature columns with more than 80% missing values,
#then removed columns with no variance. 
#I reordered columns to place key/interpretable fields.
#Saved the cleaned dataset in the folder.